{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNO567xRFKhr",
        "outputId": "5ab367bb-5206-4e2c-d390-226dcd571cea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal path found by the agent: [(0, 0), (0, 1), (1, 1), (1, 2), (2, 2)]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define constants\n",
        "GRID_SIZE = 3\n",
        "NUM_EPISODES = 1000\n",
        "MAX_STEPS = 50\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT_FACTOR = 0.9\n",
        "EPSILON = 0.1\n",
        "\n",
        "# Define actions\n",
        "ACTIONS = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "NUM_ACTIONS = len(ACTIONS)\n",
        "\n",
        "# Define states\n",
        "STATES = [(i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE)]\n",
        "NUM_STATES = len(STATES)\n",
        "\n",
        "# Define rewards and obstacles\n",
        "REWARDS = {\n",
        "    (2, 2): 10,  # Goal\n",
        "    (0, 2): -10,  # Obstacle\n",
        "}\n",
        "OBSTACLES = [(0, 2)]\n",
        "\n",
        "# Initialize Q-values\n",
        "Q_values = np.zeros((NUM_STATES, NUM_ACTIONS))\n",
        "\n",
        "# Helper functions\n",
        "def get_next_state(state, action):\n",
        "    if action == 'UP':\n",
        "        return (max(0, state[0]-1), state[1])\n",
        "    elif action == 'DOWN':\n",
        "        return (min(GRID_SIZE-1, state[0]+1), state[1])\n",
        "    elif action == 'LEFT':\n",
        "        return (state[0], max(0, state[1]-1))\n",
        "    elif action == 'RIGHT':\n",
        "        return (state[0], min(GRID_SIZE-1, state[1]+1))\n",
        "\n",
        "def select_action(state):\n",
        "    if np.random.rand() < EPSILON:\n",
        "        return np.random.choice(ACTIONS)\n",
        "    else:\n",
        "        return ACTIONS[np.argmax(Q_values[state])]\n",
        "\n",
        "# Training\n",
        "for _ in range(NUM_EPISODES):\n",
        "    state = (0, 0)\n",
        "    for _ in range(MAX_STEPS):\n",
        "        action = select_action(STATES.index(state))\n",
        "        next_state = get_next_state(state, action)\n",
        "        reward = REWARDS.get(next_state, 0)\n",
        "        if next_state in OBSTACLES:\n",
        "            next_state = (0, 0)  # Reset to start if hitting an obstacle\n",
        "        max_next_Q = np.max(Q_values[STATES.index(next_state)])\n",
        "        Q_values[STATES.index(state), ACTIONS.index(action)] += LEARNING_RATE * (reward + DISCOUNT_FACTOR * max_next_Q - Q_values[STATES.index(state), ACTIONS.index(action)])\n",
        "        state = next_state\n",
        "        if state == (2, 2):  # Reached goal\n",
        "            break\n",
        "\n",
        "# Testing\n",
        "def test_agent():\n",
        "    state = (0, 0)\n",
        "    path = [state]\n",
        "    while state != (2, 2):\n",
        "        action = ACTIONS[np.argmax(Q_values[STATES.index(state)])]\n",
        "        next_state = get_next_state(state, action)\n",
        "        if next_state in OBSTACLES:\n",
        "            next_state = (0, 0)  # Reset to start if hitting an obstacle\n",
        "        path.append(next_state)\n",
        "        state = next_state\n",
        "    return path\n",
        "\n",
        "# Analyze results\n",
        "path = test_agent()\n",
        "print(\"Optimal path found by the agent:\", path)"
      ]
    }
  ]
}